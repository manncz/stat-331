---
title: "Predictive Checks + Final Project"
format: 
  revealjs:
    theme: [default, ../slide_custom.scss]
    html-math-method: mathjax
    auto-stretch: false
editor: source
embed-resources: true
execute:
  echo: true
---

```{r setup}
#| include: false
#| message: false
library(tidyverse)
```

## Monday, March 10

Today we will...

+ Linear Regression: Feedack
+ Report Citations
+ New Material:
  + Predictive Checks
+ Work Time
  + PA 10: MPG and Horsepower?
  + PC5: Final Report

## Linear Regression: Project Feedback

:::{.incremental}

+ If you are modeling the average across years (or one particular year) make sure you include a plot of the average (or that year) in addition to the full data.
  + Show the data you are using for your linear regression.


+ If your data do not look to have a linear relationship, try transforming them!

+ If you do any **transformations**, make sure you discuss them.
  + Also make sure they are clear on any plots!
  + Your interpreation of the regression coefficients will change - let me know if you need help with this

:::

## Linear Regression: Project Feedback

:::{.incremental}

+ When you present a plot or a table, **discuss in words** what it is showing and what you want the reader to take away from it.
  + Make sure it is very clear what one point is on any plot
  + Discuss the table of variances as part of your discussion of model fit.
  
  

+ Think about the readability of the numbers you are presenting.
  + Do you need 6 decimal places?
  + Is scientific notation easily understood?

+ Include **units** on your plots!

+ Don't display the raw R `lm()` output

:::

## Report Citations

- A detailed explanation is posted [here](https://manncz.github.io/stat-331/project/proj-citations.html) and linked on Canvas.

- The short of it:
  - Include any coding references as comments in your code
  - Other citations should be in-text and in a *References* section at the end of the report
  
- A number of groups included continents in their analysis - tell me how you defined them somewhere



# Predictive Checks

> Any good analysis should include a check of the “adequacy of the fit of the model to the data and the plausibility of the model...” -- Andrew Gelman


## Predictive Checks

Predictive checks allow us to assess if our fitted model would produce data **similar to** the data that we observed.

+ Yes? Our model is a good fit.
+ No? Our model is not a good fit.

**This is an assessment of model fit.**

::: callout-caution
Predictive checks are **not** aimed to make predictions of the response variable for new observations of the explanatory variable.
:::

## Recall: Linear Regression

For simple linear regression, we assume the responses can be modeled as **a linear function of the explanatory variable and some error**.

$$y = \beta_0 + \beta_1 x_1 + \varepsilon$$

We also assume that those errors $(\varepsilon)$ follow a **normal distribution with mean 0 and standard deviation $\sigma$**.

$$\varepsilon \sim N(0, \sigma)$$


## Recall: Linear Regression

Therefore, the data **we would expect** to come from this model can be generated by:

1. predicting values from a fitted model ($\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1$) ...

&emsp; **and**

2. ... adding normally distributed errors.


## Recall: Linear Regression

This method produces data that perfectly agree with the linear model conditions:

&emsp; **L**inear relationship between $x$ and $y$.

&emsp; **I**ndependence of observations.

&emsp; **N**ormality of residuals.

&emsp; **E**qual variance of residuals.


## Predictive Checks

If we compare data generated from the linear model to the observed data, we can determine how well the observed data and linear model fit.

+ Is it plausible that the observed data could be generated by the model?


## The Process

To perform a predictive check...

1. Fit a regression model to the observed data.

2. For a set of explanatory values, obtain predicted response values from the model.

3. Repeat the following many times:

    A. Add random errors to the predictions.

    B. Compare the simulated data to the observed data.



## The Process

To perform a predictive check...

1. **Fit a regression model to the observed data.**


##

Use the `lm()` function...

```{r}
#| echo: false
#| out-width: 70%
#| fig-align: center
knitr::include_graphics("images/predchecks_1.png")
```


## The Process

To perform a predictive check...

1. Fit a regression model to the observed data.

2. **For a set of explanatory values, obtain predicted response values from the model.**


##

Use the `predict()` function...

```{r}
#| echo: false
#| out-width: 70%
#| fig-align: center
knitr::include_graphics("images/predchecks_2.png")
```


## The Process

To perform a predictive check...

1. Fit a regression model to the observed data.

2. For a set of explanatory values, obtain predicted response values from the model.

3. Repeat the following many times:

    A. **Add random errors to the predictions.**


##

Use the `rnorm()` function...

```{r}
#| echo: false
#| out-width: 55%
#| fig-align: center
knitr::include_graphics("images/predchecks_3.png")
```

The random errors have mean 0 and standard deviation estimated by the **residual standard error** (use `sigma()`).


## The Process

To perform a predictive check...

1. Fit a regression model to the observed data.

2. For a set of explanatory values, obtain predicted response values from the model.

3. Repeat the following many times:

    A. Add random errors to the predictions.

    B. **Compare the simulated data to the observed data.**


##

Use the `lm()` function to regress observed on simulated...

```{r}
#| echo: false
#| out-width: 60%
#| fig-align: center
knitr::include_graphics("images/predchecks_4.png")
```

To measure similarity, record $R^2$ (proportion of variability in $y$ explained by a linear relationship with $x$).


## The Process

To perform a predictive check...

1. Fit a regression model to the observed data.

2. For a set of explanatory values, obtain predicted response values from the model.

3. **Repeat the following many times:**

    A. Add random errors to the predictions.

    B. Compare the simulated data to the observed data.



##

Use the `map()` function to repeat the process over and over...

::::{.columns}
:::{.column width="70%"}

```{r}
#| echo: false
#| out-width: 100%
#| fig-align: center
knitr::include_graphics("images/predchecks_5.png")
```
:::
:::{.column width="30%"}
We want to see how the model performs across many simulated datasets.

+ Compute the $R^2$ for each.
:::
::::

Instead of $R^2$, could use correlation $(r)$, sum of squared errors $(SSE)$, or the estimate of $\sigma$ $(RMSE)$ to measure similarity.


## Distribution of Simulated $R^2$

Plot the distribution of simulated $R^2$ values to see how well the model performs.

+ Values distributed near 1 indicate a good fit!

```{r}
#| echo: false
#| out-width: 85%
#| fig-align: center
knitr::include_graphics("images/predchecks-5.jpg")
```


## For your project...

For your group project, you will run predictive checks to assess how well your model performs.

+ This is Section 3 of the Project Details page!


## PA 10: MPG and Horsepower?

Let's practice implementing *one iteration* of predictive checks.

![](https://media.gq.com/photos/646e3b9a29e926e42d9d858d/16:9/w_2240,c_limit/fnf.jpg){width=80%}

## To do...

+ **PA 10: MPG and Horsepower?**
  + Due **Wednesday, 3/12 at 12:00pm**.
  
+ **Course Evaluation**
  + Closes **Friday, 3/14 at 11:59pm**.

+ **Final Exam**
  + Saturday 3/15 from 1:10 - 4:00pm.
  + Alternative times as scheduled and confirmed.
  
+ **Final Project Report**
  + Due **Wednesday, 3/19 at 11:59pm**.
  
